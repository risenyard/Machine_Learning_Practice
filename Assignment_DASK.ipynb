{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed592307-32e5-4391-b502-8f4d5e843490",
   "metadata": {},
   "source": [
    "# Graded Exercise #3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fd732c-15e9-4fae-a76c-90e95664ee62",
   "metadata": {},
   "source": [
    "[EuroSAT](https://zenodo.org/records/7711810#.ZAm3k-zMKEA) is a land use and land cover classification dataset. The dataset is based on Sentinel-2 satellite imagery covering 13 spectral bands and consists of 10 Land Use and Land Cover (LULC) classes with a total of 27,000 labeled and geo-referenced images. \n",
    "\n",
    "Using the following code, you can create a DataFrame that includes columns detailing the center locations of the images and their corresponding Land Use and Land Cover (LULC) classes.\n",
    "\n",
    "Our task: Perform **supervised Machine Learning (ML)** on this dataset. Later we will use **Dask** in your implementation. We will follow the following key steps:\n",
    "- Create relevant features and analyse them using some visualizations and statistical tools. You can start with features representing the mean and range of spectral bands in these images. You are free to explore more relevant features.\n",
    "- Split the dataset into training, validation, and test sets.\n",
    "- Choose an appropriate ML algorithm.\n",
    "- Train and assess the model's performance.\n",
    "- Adjust the model's hyperparameters to optimize its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a5c7c1-1390-40ce-a14b-51a5c59b3ccf",
   "metadata": {},
   "source": [
    "## 1. Creating the work space\n",
    "Import all the relevant folders and include the file path to the where the imagery data are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d68a3821-c18c-4cbd-b5a7-11e948817e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import rasterio.warp\n",
    "from rasterio.windows import Window\n",
    "import geojson\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cc83a247-7c78-4869-8e5c-5347b1e844c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8077d511-76fa-40e6-8bbd-41af4642a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'EuroSAT_MS/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1ae1a-ffa6-44cb-bcd2-9fe03aa7c6fa",
   "metadata": {},
   "source": [
    "## 2. Creating target variable data frame and relevant features\n",
    "The goal of this excersize is to work on our understanding of machine learning concepts and try to put it into practise. Using the imagery provided by the professor we will create a dataframe with the lat, long, respective land use classification and relevant features of centroid point for each image. Land use classification is our target variable and the readings from the bands, as well as the derived features, will be used to create a model that would accurate identify land use classification in a new set up imager. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96cef0",
   "metadata": {},
   "source": [
    "While the band readings are nice, we do not need to let the machine do all the work. There are several indexes that have been proven to help identify land use classification. The following indexes will be created as additional features: \n",
    "\n",
    "1.\tNormalized Difference Vegetation Index (NDVI)- this versatile index is used in agriculture, natural hazards such as landslides, land use/land cover change detection, environmental monitoring, water resources etc. to name a few. NDVI provides valuable information in wide range of applications making it an important feature to be studied.\n",
    "NDVI = (B8-B4) / (B8+B4).\n",
    "\n",
    "2.\tSAVI- Soil Adjusted Vegetation Index (SAVI) is used to correct Normalized Difference Vegetation Index (NDVI) for the influence of soil brightness in areas where vegetative cover is low. The higher the NDVI values (the same stands for SAVI) the denser (and healthier) the vegetation. But NDVI start saturating after the value of 0.7, while SAVI at this point is only 0.3. This means that SAVI can be better used in dense vegetation because it saturates less fast. \n",
    "For Sentinel-2 the formula is:\n",
    "(B08 - B04) / (B08 + B04 + L) * (1.0 + L); L = 0.428\n",
    "where: L is a soil brightness correction factor ranging from 0 to 1\n",
    "L = 1 low vegetation cover, L = 0 high vegetation cover, L = 0.5 intermediate vegetation cover.\n",
    "https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/savi/\n",
    "\n",
    "3.\tNormalised difference water index (NDWI)- is used to highlight open water features in a satellite image, allowing a water body to “stand out” against the soil and vegetation. The downside of the index is that it is sensitive to built structures, which can lead to overestimation of water bodies.\n",
    "For Sentinel 2 data:\n",
    "NDWI= (Band 3 – Band 8)/(Band 3 + Band 8)\n",
    "NDWI: Index Formula, Value Range, And Uses In Agriculture (eos.com)\n",
    "\n",
    "The funtion below given in this section gets the NDVI, NDWI and SAVI for the centroid point of each image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "775f3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate NDVI, NDWI, and SAVI based on band values\n",
    "def calculate_indices(bands):\n",
    "    band4 = bands[3]  # Red\n",
    "    band3 = bands[2]  # Green\n",
    "    band8 = bands[7]  # NIR\n",
    "\n",
    "    # Define the formula constants for SAVI\n",
    "    L = 0.428\n",
    "    # calculate the indices\n",
    "    ndvi = (band8 - band4) / (band8 + band4)\n",
    "    ndwi = (band3 - band8) / (band3 + band8)\n",
    "    savi = (band8 - band4) / (band8 + band4 + L) * (1.0 + L)\n",
    "    return ndvi, ndwi, savi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d31de3",
   "metadata": {},
   "source": [
    "Right now we need to collect and create variables from the imagery to build out model. We wanted to include the max, median, range, and mode of all the band readings to have a broad understanding of what is going on at each observation point (centre coordinate). The funtion below reads all the tiff data and gets the data of the the centroid points of each image. The correspondentlat, lon, ndvi, ndwi, savi, max, min, range, mean and all band value are recorded for the centroid point of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8486d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read all tiff files into dataframe together with central coordinates and relevant features\n",
    "def read_data (directory_path):\n",
    "    # Build a datframe\n",
    "    data = pd.DataFrame(columns=[\"Lat\", \"Lon\", \"Lulc_class\", \"mean_val\", \"range_val\", \"median_val\", \"min_val\", \"max_val\", \"ndvi\", \"ndwi\", \"savi\",\n",
    "                             \"band1\", \"band2\", \"band3\", \"band4\", \"band5\", \"band6\", \"band7\", \"band8\", \"band9\", \"band10\", \"band11\", \"band12\"])\n",
    "    # Loop through the subfolders\n",
    "    for subdir in os.listdir(directory_path):\n",
    "        subfolder_path = os.path.join(directory_path, subdir)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "        # Get a list of all TIF files in the subfolder\n",
    "            tif_files = [file for file in os.listdir(subfolder_path) if file.endswith('.tif')]\n",
    "\n",
    "            for tif_file in tif_files:\n",
    "                tif_path = os.path.join(subfolder_path, tif_file)\n",
    "                # Read the raster file using rasterio\n",
    "                with rasterio.open(tif_path) as src:\n",
    "                    # Read and stack all bands into a single array\n",
    "                    bands = src.read()\n",
    "                    # Transfer the uint16 to int16\n",
    "                    bands = bands.astype(np.int16)\n",
    "\n",
    "                    # Get the center coordinates and  pixel location\n",
    "                    lon, lat = src.xy(src.width // 2, src.height // 2)\n",
    "                    row, col = src.index(lon, lat)\n",
    "                    bands = bands[:, row, col]\n",
    "                    # Extract LULC class from the subfolder name\n",
    "                    lulc_class = subdir\n",
    "\n",
    "                    # Calculate NDVI, NDWI, and SAVI based on band values\n",
    "                    ndvi, ndwi, savi = calculate_indices(bands)\n",
    "\n",
    "                    # Calculate the requested statistics\n",
    "                    mean_val = np.mean(bands)\n",
    "                    range_val = np.ptp(bands)\n",
    "                    median_val = np.median(bands)\n",
    "                    min_val = np.min(bands)\n",
    "                    max_val = np.max(bands)\n",
    "\n",
    "                    # Append the data to the DataFrame\n",
    "                    new_row = [lat, lon, lulc_class, mean_val, range_val,  median_val, min_val, max_val, ndvi, ndwi, savi, \n",
    "                            bands[0], bands[1], bands[2], bands[3], bands[4], bands[5], bands[6], bands[7],bands[8], bands[9], bands[10], bands[11]]\n",
    "                    data = pd.concat([data, pd.DataFrame([new_row], columns=data.columns)], ignore_index=True)\n",
    "\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9177e922-2e76-4f62-9ac9-b7852ee7c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the images\n",
    "data = read_data(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc541bd-6446-4548-8e24-053a2ed93f25",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Split the data in training, test, and validation\n",
    "For our work it was important to have separate sets for training, validation, and testing. The primary reason was to reduce over fitting. The separation between these sets ensures that the model can perform well in real-world scenarios. Using the same data for all the three purposes can lead to over fitting as the model will simply memorize the data rather than making meaningful results. We chose to do K Fold cross validations for our work as it is less biased than the simple train/test/valid split. This method of spliting data will ideally not result in overfitting, while still being relativelly simple to implement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c36a8bbd-6c76-4101-b6b8-14535d880e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c57febff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lon</th>\n",
       "      <th>Lulc_class</th>\n",
       "      <th>mean_val</th>\n",
       "      <th>range_val</th>\n",
       "      <th>median_val</th>\n",
       "      <th>min_val</th>\n",
       "      <th>max_val</th>\n",
       "      <th>ndvi</th>\n",
       "      <th>ndwi</th>\n",
       "      <th>...</th>\n",
       "      <th>band3</th>\n",
       "      <th>band4</th>\n",
       "      <th>band5</th>\n",
       "      <th>band6</th>\n",
       "      <th>band7</th>\n",
       "      <th>band8</th>\n",
       "      <th>band9</th>\n",
       "      <th>band10</th>\n",
       "      <th>band11</th>\n",
       "      <th>band12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.082000e+06</td>\n",
       "      <td>669535.179006</td>\n",
       "      <td>Forest</td>\n",
       "      <td>1051.692308</td>\n",
       "      <td>2064</td>\n",
       "      <td>841.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2074</td>\n",
       "      <td>0.559149</td>\n",
       "      <td>-0.443088</td>\n",
       "      <td>...</td>\n",
       "      <td>707</td>\n",
       "      <td>518</td>\n",
       "      <td>761</td>\n",
       "      <td>1547</td>\n",
       "      <td>1833</td>\n",
       "      <td>1832</td>\n",
       "      <td>470</td>\n",
       "      <td>10</td>\n",
       "      <td>1299</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.345699e+06</td>\n",
       "      <td>658191.991809</td>\n",
       "      <td>Forest</td>\n",
       "      <td>1501.692308</td>\n",
       "      <td>3839</td>\n",
       "      <td>823.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3849</td>\n",
       "      <td>0.786330</td>\n",
       "      <td>-0.673843</td>\n",
       "      <td>...</td>\n",
       "      <td>606</td>\n",
       "      <td>372</td>\n",
       "      <td>700</td>\n",
       "      <td>2531</td>\n",
       "      <td>3461</td>\n",
       "      <td>3110</td>\n",
       "      <td>823</td>\n",
       "      <td>10</td>\n",
       "      <td>1623</td>\n",
       "      <td>596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.305986e+06</td>\n",
       "      <td>614654.563927</td>\n",
       "      <td>Forest</td>\n",
       "      <td>1533.615385</td>\n",
       "      <td>3627</td>\n",
       "      <td>1142.0</td>\n",
       "      <td>11</td>\n",
       "      <td>3638</td>\n",
       "      <td>0.778496</td>\n",
       "      <td>-0.654965</td>\n",
       "      <td>...</td>\n",
       "      <td>688</td>\n",
       "      <td>411</td>\n",
       "      <td>834</td>\n",
       "      <td>2417</td>\n",
       "      <td>3120</td>\n",
       "      <td>3300</td>\n",
       "      <td>1205</td>\n",
       "      <td>11</td>\n",
       "      <td>1671</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.425758e+06</td>\n",
       "      <td>527278.454631</td>\n",
       "      <td>Forest</td>\n",
       "      <td>1485.384615</td>\n",
       "      <td>3691</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>9</td>\n",
       "      <td>3700</td>\n",
       "      <td>0.812892</td>\n",
       "      <td>-0.679704</td>\n",
       "      <td>...</td>\n",
       "      <td>606</td>\n",
       "      <td>328</td>\n",
       "      <td>672</td>\n",
       "      <td>2526</td>\n",
       "      <td>3383</td>\n",
       "      <td>3178</td>\n",
       "      <td>1090</td>\n",
       "      <td>9</td>\n",
       "      <td>1496</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.346349e+06</td>\n",
       "      <td>660791.255106</td>\n",
       "      <td>Forest</td>\n",
       "      <td>1672.230769</td>\n",
       "      <td>4383</td>\n",
       "      <td>925.0</td>\n",
       "      <td>11</td>\n",
       "      <td>4394</td>\n",
       "      <td>0.809548</td>\n",
       "      <td>-0.684458</td>\n",
       "      <td>...</td>\n",
       "      <td>671</td>\n",
       "      <td>377</td>\n",
       "      <td>724</td>\n",
       "      <td>2880</td>\n",
       "      <td>4037</td>\n",
       "      <td>3582</td>\n",
       "      <td>925</td>\n",
       "      <td>11</td>\n",
       "      <td>1666</td>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26995</th>\n",
       "      <td>5.426571e+06</td>\n",
       "      <td>453563.303146</td>\n",
       "      <td>Pasture</td>\n",
       "      <td>2000.923077</td>\n",
       "      <td>4842</td>\n",
       "      <td>1135.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4852</td>\n",
       "      <td>0.815224</td>\n",
       "      <td>-0.661069</td>\n",
       "      <td>...</td>\n",
       "      <td>888</td>\n",
       "      <td>443</td>\n",
       "      <td>1135</td>\n",
       "      <td>3463</td>\n",
       "      <td>4380</td>\n",
       "      <td>4352</td>\n",
       "      <td>1487</td>\n",
       "      <td>10</td>\n",
       "      <td>2184</td>\n",
       "      <td>931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26996</th>\n",
       "      <td>5.106952e+06</td>\n",
       "      <td>590250.088102</td>\n",
       "      <td>Pasture</td>\n",
       "      <td>1813.615385</td>\n",
       "      <td>3583</td>\n",
       "      <td>1419.0</td>\n",
       "      <td>11</td>\n",
       "      <td>3594</td>\n",
       "      <td>0.549563</td>\n",
       "      <td>-0.513167</td>\n",
       "      <td>...</td>\n",
       "      <td>1026</td>\n",
       "      <td>927</td>\n",
       "      <td>1419</td>\n",
       "      <td>2617</td>\n",
       "      <td>3162</td>\n",
       "      <td>3189</td>\n",
       "      <td>992</td>\n",
       "      <td>11</td>\n",
       "      <td>2877</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26997</th>\n",
       "      <td>5.969018e+06</td>\n",
       "      <td>520327.471512</td>\n",
       "      <td>Pasture</td>\n",
       "      <td>1924.538462</td>\n",
       "      <td>4593</td>\n",
       "      <td>1181.0</td>\n",
       "      <td>15</td>\n",
       "      <td>4608</td>\n",
       "      <td>0.817640</td>\n",
       "      <td>-0.673597</td>\n",
       "      <td>...</td>\n",
       "      <td>832</td>\n",
       "      <td>428</td>\n",
       "      <td>1082</td>\n",
       "      <td>3415</td>\n",
       "      <td>4299</td>\n",
       "      <td>4266</td>\n",
       "      <td>1713</td>\n",
       "      <td>15</td>\n",
       "      <td>1651</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26998</th>\n",
       "      <td>5.879766e+06</td>\n",
       "      <td>568294.733498</td>\n",
       "      <td>Pasture</td>\n",
       "      <td>2008.538462</td>\n",
       "      <td>4824</td>\n",
       "      <td>1084.0</td>\n",
       "      <td>14</td>\n",
       "      <td>4838</td>\n",
       "      <td>0.818331</td>\n",
       "      <td>-0.700402</td>\n",
       "      <td>...</td>\n",
       "      <td>783</td>\n",
       "      <td>444</td>\n",
       "      <td>1059</td>\n",
       "      <td>3395</td>\n",
       "      <td>4358</td>\n",
       "      <td>4444</td>\n",
       "      <td>2137</td>\n",
       "      <td>14</td>\n",
       "      <td>1909</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26999</th>\n",
       "      <td>5.731542e+06</td>\n",
       "      <td>431300.905643</td>\n",
       "      <td>Pasture</td>\n",
       "      <td>1670.076923</td>\n",
       "      <td>4014</td>\n",
       "      <td>966.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4024</td>\n",
       "      <td>0.743052</td>\n",
       "      <td>-0.609296</td>\n",
       "      <td>...</td>\n",
       "      <td>807</td>\n",
       "      <td>490</td>\n",
       "      <td>966</td>\n",
       "      <td>2884</td>\n",
       "      <td>3560</td>\n",
       "      <td>3324</td>\n",
       "      <td>826</td>\n",
       "      <td>10</td>\n",
       "      <td>2004</td>\n",
       "      <td>817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27000 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Lat            Lon Lulc_class     mean_val range_val   \n",
       "0      6.082000e+06  669535.179006     Forest  1051.692308      2064  \\\n",
       "1      5.345699e+06  658191.991809     Forest  1501.692308      3839   \n",
       "2      5.305986e+06  614654.563927     Forest  1533.615385      3627   \n",
       "3      5.425758e+06  527278.454631     Forest  1485.384615      3691   \n",
       "4      5.346349e+06  660791.255106     Forest  1672.230769      4383   \n",
       "...             ...            ...        ...          ...       ...   \n",
       "26995  5.426571e+06  453563.303146    Pasture  2000.923077      4842   \n",
       "26996  5.106952e+06  590250.088102    Pasture  1813.615385      3583   \n",
       "26997  5.969018e+06  520327.471512    Pasture  1924.538462      4593   \n",
       "26998  5.879766e+06  568294.733498    Pasture  2008.538462      4824   \n",
       "26999  5.731542e+06  431300.905643    Pasture  1670.076923      4014   \n",
       "\n",
       "       median_val min_val max_val      ndvi      ndwi  ...  band3 band4 band5   \n",
       "0           841.0      10    2074  0.559149 -0.443088  ...    707   518   761  \\\n",
       "1           823.0      10    3849  0.786330 -0.673843  ...    606   372   700   \n",
       "2          1142.0      11    3638  0.778496 -0.654965  ...    688   411   834   \n",
       "3          1011.0       9    3700  0.812892 -0.679704  ...    606   328   672   \n",
       "4           925.0      11    4394  0.809548 -0.684458  ...    671   377   724   \n",
       "...           ...     ...     ...       ...       ...  ...    ...   ...   ...   \n",
       "26995      1135.0      10    4852  0.815224 -0.661069  ...    888   443  1135   \n",
       "26996      1419.0      11    3594  0.549563 -0.513167  ...   1026   927  1419   \n",
       "26997      1181.0      15    4608  0.817640 -0.673597  ...    832   428  1082   \n",
       "26998      1084.0      14    4838  0.818331 -0.700402  ...    783   444  1059   \n",
       "26999       966.0      10    4024  0.743052 -0.609296  ...    807   490   966   \n",
       "\n",
       "      band6 band7 band8 band9 band10 band11 band12  \n",
       "0      1547  1833  1832   470     10   1299    629  \n",
       "1      2531  3461  3110   823     10   1623    596  \n",
       "2      2417  3120  3300  1205     11   1671    685  \n",
       "3      2526  3383  3178  1090      9   1496    587  \n",
       "4      2880  4037  3582   925     11   1666    597  \n",
       "...     ...   ...   ...   ...    ...    ...    ...  \n",
       "26995  3463  4380  4352  1487     10   2184    931  \n",
       "26996  2617  3162  3189   992     11   2877   1613  \n",
       "26997  3415  4299  4266  1713     15   1651    667  \n",
       "26998  3395  4358  4444  2137     14   1909    820  \n",
       "26999  2884  3560  3324   826     10   2004    817  \n",
       "\n",
       "[27000 rows x 23 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "83946688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input and output data\n",
    "#X = data[['mean_val', 'range_val', 'median_val', 'min_val', 'max_val', 'ndvi', 'ndwi', 'savi']]\n",
    "X = data[['band1', 'band2', 'band3', 'band4', 'band5', 'band6', 'band7', 'band8', 'band9', 'band10', 'band11', 'band12']]\n",
    "#X = data.drop(['Lulc_class'], axis=1)\n",
    "y = data['Lulc_class']\n",
    "# Create the k-fold object\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069246e0",
   "metadata": {},
   "source": [
    "## 4. Developing our model\n",
    "Random forest is a well-known machine learning model, commonly used for classification tasks. In recent studies random forest model was found to out preform artificial neural networks with the same task of land use classification [1] When working with Sentinel-2 specifically, as we are here, random forest was found to be the stand out model for land use/land cover classification [2]. For these reasons we chose random forest as our model.\n",
    "\n",
    "[1] https://www.frontiersin.org/articles/10.3389/frai.2022.964279/full#:~:text=We%20classified%20land%20use%20and,conducted%20by%20Tan%20et%20al.\n",
    "\n",
    "[2] Ge, G., Shi, Z., Zhu, Y., Yang, X., & Hao, Y. (2020). Land use/cover classification in an arid desert-oasis mosaic landscape of China using remote sensed imagery: Performance assessment of four machine learning algorithms. Global Ecology and Conservation, 22, e00971. https://www.sciencedirect.com/science/article/pii/S2351989420300202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "67a2b984-57e1-4b0a-8da0-02faf3b27b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random forest classification model\n",
    "model = RandomForestClassifier(max_depth=100, random_state=95)\n",
    "# A list for accuaracy\n",
    "acc_score = []\n",
    " \n",
    "# Get the training and testing data using the k-fold object\n",
    "for train_index , test_index in kf.split(X):\n",
    "    X_train , X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train , y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    # Train the model\n",
    "    model.fit(X_train,y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "    # Calculate the accuracy of the model\n",
    "    acc = accuracy_score(pred_values,y_test)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9f323-eeda-4554-bc42-bd5cf27f3b02",
   "metadata": {},
   "source": [
    "## 5.Hyperparameter Optimization\n",
    "- Hyperparameter optimization is the process of finding the configuration of hyperparameters that results in the best performance. Hyperparameters are the variables that control the training process and the topology of an ML model. \n",
    "- The hyperparameters of a random forest model are the number of trees in the forest, the number of features to consider when looking for the best split, the maximum depth of the tree, the minimum number of samples required to split an internal node, the minimum number of samples required to be at a leaf node, and the number of features to consider when looking for the best split. \n",
    "- We used the RandomizedSearchCV and GridSearchCV function to find the best hyperparameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3cdadfa8-c909-476e-87bd-ef544d0f92f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e7ecc",
   "metadata": {},
   "source": [
    "Randomized Search:\n",
    "- A big max_depth could produce over-fitted trees whule a small max_depth could lead to under-fitting. Considering about our feature number, the range of max_depth is set from 1 to 50.\n",
    "- Our model had around 10 features. There for the range of max_features was set to 1 to 10.\n",
    "- Min_sample_split is the minimum number of samples required to split an internal node. The range of min_sample_split was set to 2 to 10 as bigger number could lead to under-fitting and smaller number could lead to over-fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6340a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters and distributions to sample from\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# define the parameter space that will be searched over\n",
    "param_dist = {\"max_depth\": [1,2,3,4,5,10,15,20,50,None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# iterate over the training dataset multiple times\n",
    "n_iter_search = 20\n",
    "\n",
    "# run randomized search\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, cv=10)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e5aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy of RF (Random Search) is 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Run the model with best parameters\n",
    "best_random = random_search.best_estimator_\n",
    "best_random.fit(X_train, y_train)\n",
    "pred_labels = best_random.predict(X_test)\n",
    "pa_mp = accuracy_score(y_test, pred_labels, normalize=False)\n",
    "print(\"Classification accuracy of RF (Random Search) is\", pa_mp/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b2055",
   "metadata": {},
   "source": [
    "Grid Search:\n",
    "As grid search performs exhaustive search over specified parameter values for an estimator, we used the less range of values for max_depth, max_features and min_sample_split as we did for randomized search.\n",
    "- A big max_depth could produce over-fitted trees whule a small max_depth could lead to under-fitting. Considering about our feature number, the range of max_depth is set from 1 to 50.\n",
    "- Our model had around 10 features. There for the range of max_features was set to 1 to 10.\n",
    "- Min_sample_split is the minimum number of samples required to split an internal node. The range of min_sample_split was set to 2 to 10 as bigger number could lead to under-fitting and smaller number could lead to over-fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f938497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 91.73 seconds for 72 candidate parameter settings.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': None,\n",
       " 'max_features': 10,\n",
       " 'min_samples_split': 2}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the parameter space that will be searched over\n",
    "param_grid = {\"max_depth\": [1,3,5,10,20,50,None],\n",
    "              \"max_features\": [3,5,10],\n",
    "              \"min_samples_split\": [2,5,10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=10)\n",
    "start = time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c659608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy of RF (Grid Search) is 0.975925925925926\n"
     ]
    }
   ],
   "source": [
    "# Run the model with best parameters\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, X_test)\n",
    "pred_labels = best_grid.predict(y_train)\n",
    "pa_mp = accuracy_score(y_test, pred_labels, normalize=False)\n",
    "print(\"Classification accuracy of RF (Grid Search) is\", pa_mp/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d504492-e572-4bb0-9fca-b04884eea478",
   "metadata": {},
   "source": [
    "**Points of discussion**:\n",
    "- Can you explain the criteria and rationale behind the features you created? What other features you would select from these images in addition to the mean and range?\n",
    "   \n",
    "- Why is it important to have separate sets for training, validation, and testing? Which split did you consider and why?\n",
    "    \n",
    "- What factors influenced your choice of a specific machine learning algorithm?\n",
    "    \n",
    "- How did hyperparameter tuning impact the model's performance, and what were the final hyperparameter settings?\n",
    "- What is the impact of using DASK to solve this problem? What is the impact of changing DASK parameters like chunk size? You may consider checking CPU, memory usage, processing time, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
