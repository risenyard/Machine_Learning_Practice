{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed592307-32e5-4391-b502-8f4d5e843490",
   "metadata": {},
   "source": [
    "# Graded Exercise #3_ With DASK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fd732c-15e9-4fae-a76c-90e95664ee62",
   "metadata": {},
   "source": [
    "This file deals with the same process in <[Primary] Assignment_Without_DASK.ipynb> but with DASK. All the explanations, except for implementation of DASK and comparison between the results with and without DASK, are in the other file.\n",
    "DASK is used in this version to:\n",
    "- Run the machine learning model\n",
    "- Perform hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a5c7c1-1390-40ce-a14b-51a5c59b3ccf",
   "metadata": {},
   "source": [
    "## 1. Creating the work space and set up DASK\n",
    "Import all the relevant folders and include the file path to the where the imagery data are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a3821-c18c-4cbd-b5a7-11e948817e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import rasterio.warp\n",
    "import geojson\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc83a247-7c78-4869-8e5c-5347b1e844c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb55fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as da\n",
    "from dask.distributed import Client\n",
    "client = Client(processes=False, threads_per_worker=4,\n",
    "                n_workers=1, memory_limit='2GB')\n",
    "partition_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8077d511-76fa-40e6-8bbd-41af4642a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'EuroSAT_MS/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1ae1a-ffa6-44cb-bcd2-9fe03aa7c6fa",
   "metadata": {},
   "source": [
    "## 2. Creating target variable data frame and relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate NDVI, NDWI, and SAVI based on band values\n",
    "def calculate_indices(bands):\n",
    "    band4 = bands[3]  # Red\n",
    "    band3 = bands[2]  # Green\n",
    "    band8 = bands[7]  # NIR\n",
    "\n",
    "    # Define the formula constants for SAVI\n",
    "    L = 0.428\n",
    "    # calculate the indices\n",
    "    ndvi = (band8 - band4) / (band8 + band4)\n",
    "    ndwi = (band3 - band8) / (band3 + band8)\n",
    "    savi = (band8 - band4) / (band8 + band4 + L) * (1.0 + L)\n",
    "    return ndvi, ndwi, savi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8486d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read all tiff files into dataframe together with central coordinates and relevant features\n",
    "def read_data (directory_path):\n",
    "    # Build a datframe\n",
    "    data = pd.DataFrame(columns=[\"Lat\", \"Lon\", \"Lulc_class\", \"mean_val\", \"range_val\", \"median_val\", \"min_val\", \"max_val\", \"ndvi\", \"ndwi\", \"savi\",\n",
    "                             \"band1\", \"band2\", \"band3\", \"band4\", \"band5\", \"band6\", \"band7\", \"band8\", \"band9\", \"band10\", \"band11\", \"band12\"])\n",
    "    # Loop through the subfolders\n",
    "    for subdir in os.listdir(directory_path):\n",
    "        subfolder_path = os.path.join(directory_path, subdir)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "        # Get a list of all TIF files in the subfolder\n",
    "            tif_files = [file for file in os.listdir(subfolder_path) if file.endswith('.tif')]\n",
    "\n",
    "            for tif_file in tif_files:\n",
    "                tif_path = os.path.join(subfolder_path, tif_file)\n",
    "                # Read the raster file using rasterio\n",
    "                with rasterio.open(tif_path) as src:\n",
    "                    # Read and stack all bands into a single array\n",
    "                    bands = src.read()\n",
    "                    # Transfer the uint16 to int16\n",
    "                    bands = bands.astype(np.int16)\n",
    "\n",
    "                    # Get the center coordinates and  pixel location\n",
    "                    lon, lat = src.xy(src.width // 2, src.height // 2)\n",
    "                    row, col = src.index(lon, lat)\n",
    "                    bands = bands[:, row, col]\n",
    "                    # Extract LULC class from the subfolder name\n",
    "                    lulc_class = subdir\n",
    "\n",
    "                    # Calculate NDVI, NDWI, and SAVI based on band values\n",
    "                    ndvi, ndwi, savi = calculate_indices(bands)\n",
    "\n",
    "                    # Calculate the requested statistics\n",
    "                    mean_val = np.mean(bands)\n",
    "                    range_val = np.ptp(bands)\n",
    "                    median_val = np.median(bands)\n",
    "                    min_val = np.min(bands)\n",
    "                    max_val = np.max(bands)\n",
    "\n",
    "                    # Append the data to the DataFrame\n",
    "                    new_row = [lat, lon, lulc_class, mean_val, range_val,  median_val, min_val, max_val, ndvi, ndwi, savi, \n",
    "                            bands[0], bands[1], bands[2], bands[3], bands[4], bands[5], bands[6], bands[7],bands[8], bands[9], bands[10], bands[11]]\n",
    "                    data = pd.concat([data, pd.DataFrame([new_row], columns=data.columns)], ignore_index=True)\n",
    "\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177e922-2e76-4f62-9ac9-b7852ee7c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the images\n",
    "data = read_data(directory_path)\n",
    "# Make the dataframe DASK dataframe\n",
    "data_dask = da.from_pandas(data, npartitions=partition_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc541bd-6446-4548-8e24-053a2ed93f25",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Split the data in training, test, and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a8bbd-6c76-4101-b6b8-14535d880e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask_ml.preprocessing as pr\n",
    "# The indexing in Dask Dataframe is not supported, so we need to give up the KFold\n",
    "# from sklearn.model_selection import KFold\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83946688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input and output data\n",
    "#X = data[['mean_val', 'range_val', 'median_val', 'min_val', 'max_val', 'ndvi', 'ndwi', 'savi']]\n",
    "X = data_dask[['band1', 'band2', 'band3', 'band4', 'band5', 'band6', 'band7', 'band8', 'band9', 'band10', 'band11', 'band12']]\n",
    "#X = data.drop(['Lulc_class'], axis=1)\n",
    "y = data_dask['Lulc_class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069246e0",
   "metadata": {},
   "source": [
    "## 4. Developing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a2b984-57e1-4b0a-8da0-02faf3b27b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random forest classification model\n",
    "model = RandomForestClassifier(max_depth=5, random_state=95)\n",
    "# A list for accuaracy\n",
    "acc_score = []\n",
    " \n",
    "\n",
    "# Get the training and testing data using the k-fold object\n",
    "for _ in range(partition_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    # Train the model\n",
    "    model.fit(X_train,y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "    # Calculate the accuracy of the model\n",
    "    acc = accuracy_score(pred_values,y_test)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/partition_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3646fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_acc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9f323-eeda-4554-bc42-bd5cf27f3b02",
   "metadata": {},
   "source": [
    "## 5.Hyperparameter Optimization with DASK\n",
    "- We used the RandomizedSearchCV and GridSearchCV function to find the best hyperparameters for our model.\n",
    "(HyperbandSearchCV does not work with random forest classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "51a06e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "#from dask_ml.model_selection import HyperbandSearchCV\n",
    "import dask_ml.model_selection as dcv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032ec488",
   "metadata": {},
   "source": [
    "Randomized Search:\n",
    "- A big max_depth could produce over-fitted trees whule a small max_depth could lead to under-fitting. Considering about our feature number, the range of max_depth is set from 1 to 50.\n",
    "- Our model had around 10 features. There for the range of max_features was set to 1 to 10.\n",
    "- Min_sample_split is the minimum number of samples required to split an internal node. The range of min_sample_split was set to 2 to 10 as bigger number could lead to under-fitting and smaller number could lead to over-fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "093f38a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 380.28 seconds for 10 candidates parameter settings.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': 50,\n",
       " 'max_features': 6,\n",
       " 'min_samples_split': 5}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify parameters and distributions to sample from\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# define the parameter space that will be searched over\n",
    "param_dist = {\"max_depth\": [1,2,3,4,5,10,15,20,50,None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# iterate over the training dataset multiple times\n",
    "n_iter_search = 10\n",
    "\n",
    "# run randomized search\n",
    "random_search = dcv.RandomizedSearchCV(clf, param_dist,n_iter_search, cv=10)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "\n",
    "random_search.best_params_       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eb774022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy of RF (Random Search) is 0.7591047040971168\n"
     ]
    }
   ],
   "source": [
    "# Run the model with best parameters\n",
    "best_random = random_search.best_estimator_\n",
    "best_random.fit(X_train, y_train)\n",
    "pred_labels = best_random.predict(X_test)\n",
    "pa_mp = accuracy_score(y_test, pred_labels, normalize=False)\n",
    "print(\"Classification accuracy of RF (Random Search) is\", pa_mp/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded2eb25",
   "metadata": {},
   "source": [
    "Grid Search:\n",
    "As grid search performs exhaustive search over specified parameter values for an estimator, we used the less range of values for max_depth, max_features and min_sample_split as we did for randomized search.\n",
    "- A big max_depth could produce over-fitted trees whule a small max_depth could lead to under-fitting. Considering about our feature number, the range of max_depth is set from 1 to 50.\n",
    "- Our model had around 10 features. There for the range of max_features was set to 1 to 10.\n",
    "- Min_sample_split is the minimum number of samples required to split an internal node. The range of min_sample_split was set to 2 to 10 as bigger number could lead to under-fitting and smaller number could lead to over-fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "90cb6c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-03 23:04:21,930 - tornado.application - ERROR - Exception in callback <bound method SystemMonitor.update of <SystemMonitor: cpu: 394 memory: 177 MB fds: 465>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yokosukamori/Library/Python/3.9/lib/python/site-packages/tornado/ioloop.py\", line 921, in _run\n",
      "    val = self.callback()\n",
      "  File \"/Users/yokosukamori/Library/Python/3.9/lib/python/site-packages/distributed/system_monitor.py\", line 179, in update\n",
      "    disk_ioc = psutil.disk_io_counters()\n",
      "  File \"/Users/yokosukamori/Library/Python/3.9/lib/python/site-packages/psutil/__init__.py\", line 2064, in disk_io_counters\n",
      "    rawdict = _psplatform.disk_io_counters(**kwargs)\n",
      "RuntimeError: unable to get the disk's parent.\n"
     ]
    }
   ],
   "source": [
    "# define the parameter space that will be searched over\n",
    "param_grid = {\"max_depth\": [1,3,5,10,20,50,None],\n",
    "              \"max_features\": [3,5,10],\n",
    "              \"min_samples_split\": [2,5,10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = dcv.GridSearchCV(clf, param_grid, cv=10)\n",
    "start = time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a78e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy of RF (Grid Search) is 0.975925925925926\n"
     ]
    }
   ],
   "source": [
    "# Run the model with best parameters\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, X_test)\n",
    "pred_labels = best_grid.predict(y_train)\n",
    "pa_mp = accuracy_score(y_test, pred_labels, normalize=False)\n",
    "print(\"Classification accuracy of RF (Grid Search) is\", pa_mp/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54b6046",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
